---
layout: post
title: "VERSA: Revolutionizing Speech and Audio Evaluation"
description: VERSA: A Comprehensive Speech and Audio Evaluation Toolkit
date: 2025-04-25
comments: false

authors:
  - name: Jiatong Shi
    url: "http://shijt.site"
    affiliations:
      name: Carnegie Mellon University
---

{% include figure.html path="assets/img/versa-light-char.png" class="img-fluid rounded z-depth-0" zoomable=true %}

# Introducing VERSA: A Comprehensive Speech and Audio Evaluation Toolkit

The WavLab team is excited to announce the public release of **VERSA** (Versatile Evaluation of Speech and Audio), our comprehensive toolkit designed to revolutionize how researchers and developers evaluate speech and audio quality.

## Why We Built VERSA

Audio quality assessment has long been fragmented across numerous specialized metrics, each requiring different setups, dependencies, and formats. This fragmentation creates significant barriers for researchers and practitioners alike:

- Implementing multiple metrics requires navigating various codebases
- Comparing results across different papers becomes challenging due to inconsistent implementations
- Setting up evaluation pipelines consumes valuable research time
- Reproducing published results is often unnecessarily difficult

VERSA solves these problems by providing a unified framework that brings together over 80 evaluation metrics under a single, easy-to-use interface.

## What Sets VERSA Apart

### Comprehensive Coverage

VERSA provides seamless access to more than 80 evaluation and profiling metrics with 10x variants, covering:

- **Perceptual quality** metrics that correlate with human perception
- **Intelligibility** measures critical for speech applications
- **Technical measurements** for detailed audio analysis
- **Distributional metrics** to evaluate statistical properties across collections

### Practical Benefits

- **Integrate once, access everything**: No need to implement multiple evaluation tools
- **Consistent interface**: Evaluate different aspects of audio quality through a unified API
- **Flexible input support**: Works with file paths, SCP files, and Kaldi-style ARKs
- **Built for scale**: Distributed evaluation support with Slurm integration
- **ESPnet compatibility**: Tightly integrated with the popular ESPnet framework

## Getting Started

### Installation

```bash
git clone https://github.com/wavlab-speech/versa.git
cd versa
pip install .
```

For metrics requiring additional dependencies, our `tools` directory provides convenient installers.

### Quick Example

Evaluating speech quality is as simple as:

```bash
python versa/bin/scorer.py \
    --score_config egs/speech.yaml \
    --gt /path/to/reference/audio \
    --pred /path/to/generated/audio \
    --output_file results \
    --io dir
```

## Real-World Applications

VERSA enables researchers and developers to:

- **Benchmark speech enhancement systems** across multiple quality dimensions
- **Evaluate text-to-speech models** for naturalness and intelligibility
- **Assess audio codec performance** with comprehensive quality metrics
- **Compare speech recognition outputs** with integrated WER and other metrics
- **Profile audio datasets** for statistical properties and quality issues

## Interactive Demo

Want to see VERSA in action? Try our interactive demo from our Interspeech 2024 Tutorial:
[Colab Demonstration](https://colab.research.google.com/drive/11c0vZxbSa8invMSfqM999tI3MnyAVsOp?usp=sharing)

## Join the Community

VERSA is open-source and community-driven. We welcome contributions and feedback:

- **Star us on GitHub**: [![GitHub stars](https://img.shields.io/github/stars/wavlab-speech/versa?style=social)](https://github.com/wavlab-speech/versa/stargazers)
- **Report issues** or suggest features on our [GitHub repository](https://github.com/wavlab-speech/versa)
- **Contribute**: Check our [contributing guidelines](https://github.com/wavlab-speech/versa/blob/main/docs/contributing.md)

## Citation

If you find VERSA useful in your research, please cite our papers:

```bibtex
@inproceedings{shi2025versa,
title={{VERSA}: A Versatile Evaluation Toolkit for Speech, Audio, and Music},
author={Jiatong Shi and Hye-jin Shim and Jinchuan Tian and Siddhant Arora and Haibin Wu and Darius Petermann and Jia Qi Yip and You Zhang and Yuxun Tang and Wangyou Zhang and Dareen Safar Alharthi and Yichen Huang and Koichi Saito and Jionghao Han and Yiwen Zhao and Chris Donahue and Shinji Watanabe},
booktitle={2025 Annual Conference of the North American Chapter of the Association for Computational Linguistics -- System Demonstration Track},
year={2025},
url={https://openreview.net/forum?id=zU0hmbnyQm}
}

@inproceedings{shi2024versaversatileevaluationtoolkit,
  author={Shi, Jiatong and Tian, Jinchuan and Wu, Yihan and Jung, Jee-Weon and Yip, Jia Qi and Masuyama, Yoshiki and Chen, William and Wu, Yuning and Tang, Yuxun and Baali, Massa and Alharthi, Dareen and Zhang, Dong and Deng, Ruifan and Srivastava, Tejes and Wu, Haibin and Liu, Alexander and Raj, Bhiksha and Jin, Qin and Song, Ruihua and Watanabe, Shinji},
  booktitle={2024 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs For Audio, Music, and Speech}, 
  year={2024},
  pages={562-569},
  doi={10.1109/SLT61566.2024.10832289}
}
```

## Looking Forward

We're committed to continuously improving VERSA with new metrics, enhanced usability, and expanded documentation. Stay tuned for upcoming features and don't hesitate to reach out with suggestions!
