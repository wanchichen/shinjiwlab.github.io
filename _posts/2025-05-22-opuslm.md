---
layout: distill
title: Towards Intelligent Conversational AI
description: A summary of our recent work on making speech language models smarter.
date: 2025-05-22
comments: false

authors:
  - name: Siddhant Arora*
    url: "https://siddhu001.github.io/"
    affiliations:
      name: Carnegie Mellon University
  - name: William Chen*
    url: "https://wanchichen.github.io"
    
  - name: Jinchuan Tian*
    url: "https://jctian98.github.io/"
    
  - name: Jiatong Shi
    url: "http://shijt.site"
    
  - name: Shinji Watanabe
    url: "https://sites.google.com/view/shinjiwatanabe"

bibliography: 2025-05-22-opuslm.bib

toc:
  - name: "OpusLM - A Generative Foundation Model for Speech and Text"
  - name: "Chain-of-Thought Dialogue Modelling"
  - name: "OpusChat - A Speech LM that can Remember and Reason"
  - name: "OpusTTS - Scaling Zero-Shot TTS"
  - name: "Development and Engineering"

---

# Introduction

Our papers on OpusLM and Chain-of-Thought Multi-Modal Training will be presented at [Interspeech 2025](https://www.interspeech2025.org/home) in Rotterdam. Come check out our presentations to learn more!

# OpusLM - A Generative Foundation Model for Speech and Text

{% details Authors %}
*Jinchuan Tian, William Chen, Yifan Peng, Jiatong Shi, Siddhant Arora, Shikar Bharadwaj, Takashi Maekaku, Yusuke Shinohara, Keita Goto, Xiang Yue, Chao-Han Huck Yang, Shinji Watanabe*
{% enddetails %}
&nbsp;

[Model]() | [Paper]() | [Demo]()

We first present OpusLM, a family of backbone foundation models (135M-7B parameters) for generation and understanding speech/text, trained on ASR, TTS, Text Continuation, and Speech Continuation. OpusLM was developed with the goal of tackling two key  challenges in SpeechLM development: 1.) the input/output representation of speech and 2.) degradation in text-only tasks. By addressing both of these issues, OpusLM is able to obtain SOTA performance in both ASR and TTS, while maintaining strong scores on text-benchmarks like MMLU.

## Speech Tokenization

Text-based LLMs operate on sequences of tokens, discrete integers that represent words or subwords. To fully take advantage of text-based pre-training, we must also represent speech in the same way. This is usually done through the use of quantized speech embeddings (from a mode like wav2vec2 or Whisper) or neural codecs. The two methods, however, have a clear tradeoff between *compression* and *learnability*. Codecs excel at compressing and un-compressing audio, but their representations are complex for LLMs to model. On the other hand, speech encoders have easy-to-learn representations that are well-aligned with human phonetics, but at the cost of discarding much of the information necessary for reconstruction.

A common solution is to distill the knowledge of the speech encoder into the codec (cite moshi, speech tokenizer). However, this leads to the drawback of slower codec training while still not being able to match the performance of the audio encoder in semantic/understanding-based tasks. We use a much simpler method yet more effective for OpusLM: we use both models for tokenization and feed them into the LLM as separate input/output streams. In total, this leads to 9 audio representation streams. Like Moshi, we introduce a delay of 1 step between each audio token, allowing the model to understand audio in a course-to-fine-grained manner.

{% include figure.html path="assets/img/blog/arch.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
Architecture OpusLM. We use XEUS as the speech encoder and a custom version of DAC as the neural codec.
</div>

With these techniques, we are able to achieve *enhanced* if not equal performance on both ASR and TTS from our multi-task training, allowing us to perform both speech generation and understanding with a single model. 

## Text LM Degradation

If you have ever tried the [Moshi Demo](), you will know that the model produces high-quality conversational speech at near-human speeds. But have you ever noticed that the model can be at times, shockingly dumb? Many people have observed that training LLMs on speech-text data significantly degrades text-only performance, likely due to the conversational nature of speech: the LLM is forgetting all the world knowledge that it acquired from Wikipedia and textbooks! We find that with a lot of care, however, this can be mitigated almost completely. 

{% include figure.html path="assets/img/blog/mmlu.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
MMLU scores compared to SOTA LLMS and SpeechLMs.
</div>

We first look into the training dynamics of SpeechLMs - when during training is the model starting to forget? Interestingly, we find that it correlates completely with learning rate. The forgetting primarily occurs when the learning rate increases, such as the initial warmup stage - scores on MMLU (one of the main LLM benchmarks) drop significantly as the learning rate reaches its peak. As the learning rate decreases during the decay step, MMLU scores begin to recover. To further recover text performance, we leverage the annealing/mid-training stage of LLMs - the point in pre-training where the learning rate is rapidly decayed to zero while training on high-quality data. Finally, we can almost fully recover text performance by integrating  the *exact* same text corpora as the original LLM during annealing. Our final scores are shown in the Figure below. OpusLM 7B achieves only a small 0.9 drop in MMLU from its OLMo2 backbone after training on speech, making it signifcantly smarter than Moshi or SpiritLM.  

# Chain-of-Thought Dialogue Modelling

{% details Authors %}
*Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi,, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe*
{% enddetails %}
&nbsp;

[Model]() | [Paper]() | [Demo]()

One of the most common use cases for SpeechLMs is conversational dialogue agents. Spoken dialogue, however, is very difficult to model. Despite recent progress, the inability to generate intelligent and coherent spoken responses is a major limitation of open SpeechLMs. We attribute this to two main challenges: lack of conversational training data and the inability to leverage textual knowledge. This work focuses on the latter issue and develops a solution with a common technique from NLP: Chain-of-Thought (CoT) reasoning.

{% include figure.html path="assets/img/blog/mmlu.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
CoT Formulation for Spoken Dialogue.
</div>

A common implementation of Spoken Dialogue Systems is through a fully speech-to-speech SpeechLM: the system receives a spoken utterance, and then generates a response. This method is simple and efficient, only consuming the compute necessary to complete the task. However, we hypothesized that such a formulation was sub-optimal: the system could not efficiently leverage the underlying LLM's world knowledge. CoT provides a solution: we allow the model to generate and reason about a text response, before generating the output speech. This forumlation is similar to traditional ASR + Dialogue + TTS cascades, but within a single end-to-end model that is immune to error propogation.

Our results are straighforward: CoT dialogue training led to higher quality responses than speech-to-speech and cascaded systems. The textual reasoning allowed for more coherent responses compared to the speech-to-speech model, and access to the original input speech allowed the CoT model to handle emotions and paralinguistics that the cascade was not capable of.

# OpusChat - A Speech LM that can Remember and Reason

{% details Authors %}
*Jinchuan Tian, Bo-Hao Su, Siddhant Arora, Jiatong Shi, William Chen, Keita Goto, Takashi Maekaku, Yusuke Shinohara, Chao-Han Huck Yang, Shinji Watanabe*
{% enddetails %}
&nbsp;

[Model]() | [Paper]() | [Demo]()

We apply the lessons learned in the previous papers to develop OpusChat, an intelligent SpeechLM that remembers world knowledge and can reason before responding. We first further enhanced the textual capabilities of OpusLM 7B with additional training and annealing, leading to better knowledge retainment. Across 9 common LLM benchmarks, our new OpusLM 7B v2 achieves an average score of 62.1, nearly the same as its OLMo-2-7B backbone, and therefore much more performant than Moshi or SpiritLM.

{% include figure.html path="assets/img/blog/mmlu.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
Text LM Benchmark Scores.
</div>

We then collected text-only instruction fine-tuning data to supervised fine-tuning, but realized that they were unsuitable for speech applications. Much of the instruction-tuning data for LLMs is designed for textual interfaces, with the output response organized into code, equations, tables, and bullet points. None of these are present in spoken language! To address this, we use LLMs to perform colloquial re-writing of the instruction-tuning data, transforming it to be much more conversational and casual. Finally, we synthesize spoken variants of the instruction-tuning data with our TTS-trained OpusLM, totalling to 240k instruction pairs which we use for CoT fine-tuning.

{% include figure.html path="assets/img/blog/mmlu.png" class="img-fluid rounded z-depth-0" zoomable=true %}
<div class="caption">
OpusChat reasoning before speaking.
</div>

After CoT training, we find that OpusLM can perform intelligent speech-based tasks (such as spoken math, dialogue, spoken trivia) at a very high level, out-performing SOTA foundation models like Qwen2-Audio, Moshi, GLM-4-Voice, and Baichuan-Omni.

# OpusTTS - Scaling Zero-Shot TTS 

{% details Authors %}
*William Chen, Jinchuan Tian, Chao-Han Huck Yang, Shinji Watanabe*
{% enddetails %}
&nbsp;

[Model]() | [Paper]() | [Demo]()

# Engineering and Development

We developed the models using the [ESPnet-SpeechLM]() and/or the [ESPnet-SDS]() codebases, with much of the evaluation done through the [VERSA]() evaluation toolkit. These tools were designed so that SpeechLM research can be done in both a scalable and efficient manner, allowing academic labs like us to punch above our weight for large-scale training. 

For example, OpusTTS took only 4 days of training on 16 GH200 GPUs.

# Training, Fine-Tuning, and Inference on Custom Data

All of these models can be downloaded and perform inference from www.github.com/espnet/opuslm, with only a single pip install and a few lines of code.

For those interested in training and fine-tuning, stay tuned! Complete training pipelines and code will be merged into the main ESPnet repository soon.

# Future Work